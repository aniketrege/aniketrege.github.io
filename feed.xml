<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aniketrege.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://aniketrege.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-06-17T03:50:07+00:00</updated><id>https://aniketrege.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Human Creativity and AI Creation</title><link href="https://aniketrege.github.io/blog/2024/ai-creativity/" rel="alternate" type="text/html" title="Human Creativity and AI Creation" /><published>2024-11-09T00:00:00+00:00</published><updated>2024-11-09T00:00:00+00:00</updated><id>https://aniketrege.github.io/blog/2024/ai-creativity</id><content type="html" xml:base="https://aniketrege.github.io/blog/2024/ai-creativity/"><![CDATA[<h2 id="what-does-creativity-mean-to-you">What Does Creativity Mean to You?</h2>
<p>My goal designing these posters was to <em>compare and contrast</em> my understanding of the <code class="language-plaintext highlighter-rouge">human and AI creation process</code> with simple analogies of where humans enjoy expressing their creativity. I chose art and music because I find myself observing a huge range of emotions when consuming art and music (though I personally struggle to be creative in these areas). I also chose cooking, as I feel able to express myself creatively through playing around with recipes (the gratification being instantly consumable is also quite satisfying).</p>

<p>Through the visualizations below, I hope to encourage readers to consider and contemplate my position in the AI creativity debate<d-cite key="roose2022ai, cain2022capitol"></d-cite>: similar to philosopher Sean Dorrance Kelly<d-cite key="kelly2019philosopher"></d-cite>, I believe that <code class="language-plaintext highlighter-rouge">while AI can create and be a tool for creation, it cannot be truly creative</code> and evoke a sense of true wonder and emotion.</p>

<center><img src="/assets/img/blog/ai_creativity/AI_Creativity_PartOne.jpg" alt="How does the human creative process differ from the AI creation process?" width="900" height="1122" /></center>
<p><br /><br /></p>
<center><img src="/assets/img/blog/ai_creativity/AI_Creativity_PartTwo.jpg" alt="How does the human creative process differ from the AI creation process?" width="900" height="618" /></center>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to Prof. <a href="https://www.annette-zimmermann.com/">Annette Zimmermann</a> and students from their excellent <a href="https://guide.wisc.edu/courses/philos/">Philos 543</a> course at UW-Madison (Fall ‘24) for thought provoking conversations about the ethics of AI and creativity. Also thanks to <a href="https://harshali.in/about">Harshali Paralikar</a> for discussions about what creativity means to her, and help designing these posters!</p>]]></content><author><name>Aniket Rege</name></author><category term="ai" /><category term="creativity" /><category term="art" /><category term="music" /><category term="cooking" /><summary type="html"><![CDATA[My thoughts about what it means to be creative, and why I believe AI falls short]]></summary></entry><entry><title type="html">Matryoshka Representation Learning (MRL) from the Ground Up</title><link href="https://aniketrege.github.io/blog/2024/mrl/" rel="alternate" type="text/html" title="Matryoshka Representation Learning (MRL) from the Ground Up" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-07T00:00:00+00:00</updated><id>https://aniketrege.github.io/blog/2024/mrl</id><content type="html" xml:base="https://aniketrege.github.io/blog/2024/mrl/"><![CDATA[<blockquote>
  <p>This article is best suited for ML practitioners and researchers, but is written in an attempt to be accessible to anyone interested in machine learning, artificial intelligence, and tech.
Reading Time Estimates for the entire article:</p>
  <ol>
    <li>ML Practitioner: 25 minutes</li>
    <li>Working in Computer Science/Tech: 40 minutes</li>
    <li>Generally Interested: 1 hour+</li>
  </ol>
</blockquote>

<h3 id="edits">Edits</h3>
<ol>
  <li>Feb. 14, 2024: added new detailed Sections for <a href="#why-does-mrl-work">Why Does MRL Work</a> and <a href="#adaptive-retrieval">Adaptive Retrieval</a>. Also added additional details behind creating MRL with <a href="#ml-researcher-bonus-inklings-of-nested-codebook-subspaces">nested low dimensional codebooks</a> and links to excellent <a href="#further-reading">blogs</a> and <a href="#open-source-mrl-models">products</a> using MRL!</li>
  <li>Apr. 29, 2024: I gave a talk at UW Madison’s MLOPT idea seminar about MRL, which is <a href="https://www.youtube.com/watch?v=IbfdvzPwZTg">up on Youtube</a>!</li>
</ol>

<h1 id="what-is-mrl">What is MRL?</h1>
<p>If you keep yourself updated on OpenAI’s blog posts, you may have seen the recent release of <a href="https://openai.com/blog/new-embedding-models-and-api-updates">new embedding models</a>, which included support for <em>shortening embeddings</em>, where developers could simply “remove some numbers from the end of a sequence” and still maintain a valid representation for text. Why is this cool?</p>
<ol>
  <li>Save a lot of memory (storing the embedding)</li>
  <li>Improved Search latency (smaller embeddings = faster search)</li>
  <li><strong>Critical</strong>: What if the biggest embedding isn’t the best? And what does <em>the best</em> even mean?</li>
</ol>

<center>
<blockquote class="twitter-tweet" data-theme="dark"><p lang="en" dir="ltr">Update: thanks <a href="https://twitter.com/owencm?ref_src=twsrc%5Etfw">@owencm</a> and <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> for clarifying that shortening embeddings are based on MRL and thanks for sending the blog for edit. Many thanks to <a href="https://twitter.com/owencm?ref_src=twsrc%5Etfw">@owencm</a> for being super generous in their response ♥️ We all win by pushing open research and appropriate attribution! <a href="https://t.co/GbJBgzWwO5">https://t.co/GbJBgzWwO5</a></p>&mdash; Prateek Jain (@jainprateek_) <a href="https://twitter.com/jainprateek_/status/1751479439052140622?ref_src=twsrc%5Etfw">January 28, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p>This property emerged in the new models due to a mysterious and unspecified “technique”. After some <a href="https://twitter.com/jainprateek_/status/1751479439052140622">very minor subtweeting</a>, OpenAI kindly <a href="https://twitter.com/owencm/status/1751409104713826666">updated their blog post</a> (see above) to remove this shroud of mystery to reveal this technique: :nesting_dolls: Matryoshka Representation Learning<d-cite key="kusupati2022matryoshka"></d-cite> :nesting_dolls:, which you should fully understand from this single GIF:</p>

<center><img src="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExdXNscHUzajM3ejVhbjR5dXJwczE4N2Y4a28wc3plNW9ucjRmN25jZyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/nthoYgQ91Up2u7qmcE/giphy.gif" alt="A gif illustrating Matroyshka (Russian) dolls" width="300" height="300" /></center>

<h4 id="what">What?</h4>
<p>You didn’t understand a lot of what you just read? That’s okay, let’s take a step back.</p>

<blockquote>
  <p>Note: For ML Researchers or those familiar with this space, feel free to skip directly to the <a href="#matryoshka">Matryoshka</a> :nesting_dolls: section.</p>
</blockquote>

<h2 id="representation-learning">Representation Learning</h2>
<h4 id="tldr">tl;dr</h4>
<p>Modern Deep Learning allows us to <em>learn</em> good representations for data, instead of having experts handcraft them. The <a href="#matryoshka">Matryoshka Section</a> will take closer look at what “good” means.</p>

<h3 id="how-do-we-represent-data-for-computers">How do we Represent Data for Computers?</h3>
<p>Let’s say you want to build something cool using some data that you have - a fairly general problem we all face every day. With computers, we need a way to represent this data - an image, text, data table, audio - in a way computers can understand. Computers understand data with numbers, and we typically arrive at these numbers with some function \(\textcolor{gray}{f}\) that maps the data from its original representation (e.g. an <a href="https://web.stanford.edu/class/cs101/image-1-introduction.html#:~:text=Each%20of%20the%20red%2C%20green,in%20a%20shade%20of%20orange">RGB encoding of an image</a>) to a sequence of \(\textcolor{gray}{d}\) numbers, which we call the <code class="language-plaintext highlighter-rouge">learned representation</code> \(\textcolor{gray}{z}\). In <em>math language</em>  we say \(\textcolor{gray}{z\in\mathbb{R}^d}\), or representation \(\textcolor{gray}{z}\) belongs to the set of <a href="https://en.wikipedia.org/wiki/Real_number">real numbers</a> \(\textcolor{gray}{\mathbb{R}}\), with <code class="language-plaintext highlighter-rouge">dimensionality</code> \(\textcolor{gray}{d}\).</p>

\[\begin{align*}
&amp;x = \text{cat image} \\
&amp;z = f(x) = [\text{num}_1, \text{num}_2, \text{num}_3, ..., \text{num}_d] \\
&amp;e.g.~z = [0.42, -1.96, ..., 1.43],~z \in \mathbb{R}^d \\
\end{align*}\]

<p>I know that was a lot of notation, so I hope you’re still with me! Take a minute to understand this process, with the help of the <a href="#what-is-machine-learning">diagram below</a> for a visual overview.</p>

<p>Now how do we pick a good function \(\textcolor{gray}{f}\) to represent our cat image? In the “old days” (if you’ve read ML research before 2012, I’m crying with you), expert humans would have done the Representation Learning for everyone else, i.e. used a PhD-worth of domain-specific knowledge to hand-craft good features to represent a cat. For example, maybe we care about</p>
<ol>
  <li>Horizontal and vertical edges (<a href="https://en.wikipedia.org/wiki/Canny_edge_detector">Canny Edge Detector</a>)</li>
  <li>Image texture (<a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor Filter</a>)</li>
  <li>Something super fancy sounding (<a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">Histogram of Oriented Gradients</a> - HoG)</li>
</ol>

\[\begin{align*}
&amp;f\to \text{can be HoG, Canny, Gabor} \\
&amp;z = \text{HoG}(x),~\text{z has local object appearance and shape information} \\
&amp;z = \text{Canny}(x),~\text{z has vertical and horizontal edge information} \\
&amp;z = \text{Gabor}(x),~\text{z has texture pattern information}
\end{align*}\]

<p>There’s one slight problem, what do we do if we aren’t a domain expert with years of research experience? Enter this story’s main character: <code class="language-plaintext highlighter-rouge">Machine Learning</code>.</p>

<h3 id="what-is-machine-learning">What is Machine Learning?</h3>

<p>What if we could let a machine <em>learn</em> this function \(\textcolor{gray}{f}\) from the data? That way, we wouldn’t need image processing PhDs to come up with fancy specialized featurizers (like Canny, Gabor, and HoG) that, while we understand what they are doing, they don’t generally work well for new kinds of data.</p>

<details><summary>Click here to know more about the History of Learning \(\textcolor{gray}{f}\) from Data</summary>
<p>For Images, this function class \(\textcolor{gray}{f}\) was dominated for a long time by <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a>(CNNs), after the CNN <a href="https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20the%20name%20of,D.">AlexNet</a> kicked off the Deep Learning revolution in 2012. The introduction of the <a href="https://huggingface.co/learn/nlp-course/en/chapter1/4">Transformer architecture</a><d-cite key="vaswani2017attention"></d-cite>, which revolutionized machine learning for text data in 2017, made its way to the image domain in 2021 with Google’s <a href="https://huggingface.co/docs/transformers/en/model_doc/vit">Vision Transformer</a><d-cite key="dosovitskiy2020image"></d-cite> work. These modern Deep Learning methods are also called <code class="language-plaintext highlighter-rouge">Neural Encoders</code> as they learn an encoding of the data that computers can work with using Neural Networks.</p>

<p>If you’d like to learn more about how popular Neural Encoders learn good functions \(\textcolor{gray}{f}\), I heartily recommend an excellent series of blogs from <a href="https://twitter.com/JayAlammar">Jay Alammar</a>, especially the basics of <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/">Neural Networks</a> and the <a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a>.</p>
</details>

<center><img src="/assets/img/blog/mrl/searching_for_functions.png" alt="A simple mechanism to use a learning algorithm to search the function space for a good representation of data" width="600" height="300" /></center>
<p><br /><br /></p>

<p>This is great, now we can offload the expertise of crafting features to a neural network that can learn an encoder function \(\textcolor{gray}{f}\). As seen in the figure above, this workflow involves a learning algorithm \(\textcolor{gray}{\mathcal{A}}\) that is able to search the space of all possible functions \(\textcolor{gray}{\mathcal{F}}\) to learn (hopefully) a good representation of the data \(\textcolor{gray}{z=f(x)}\), i.e. a sequence of \(\textcolor{gray}{d}\) numbers our computer can understand.</p>

<p>But what does it mean to learn a “good” representation \(\textcolor{gray}{z}\)? This question was the inspiration for <code class="language-plaintext highlighter-rouge">Matryoshka</code>.</p>

<h2 id="practical-ml-training">Practical ML Training</h2>
<h4 id="tldr-1">tl;dr</h4>
<p>We use proxy “objective functions” to train our models with the hope that they achieve our actual goals.</p>

<h3 id="how-do-we-train-machine-learning-models">How do we Train Machine Learning Models?</h3>
<p>We train modern Machine Learning models (since 2012, these are typically “Deep”, i.e. very large, and always getting larger!) with some human interpretable goal. For example, we may want a Face Recognition Model to correctly identify employees of a business entering the premises with 99% accuracy. How do we train a model to achieve this? In ML land, we use an objective function, or “loss” to steer our initial (bad) model to update itself iteratively and hopefully do slightly better each time until we hit the 99% accuracy we require.</p>

<details><summary>Click here to know more about Optimization, the Study of how to Train Good ML Models</summary>
<p>There exists rich literature in optimization, the study of how to train machine learning models well, which typically means with some guarantees on performance. With modern Deep Learning methods, these theoretical guarantees become trickier to achieve, and yet they seem to empirically work well with lots of good quality data and scale. The prevalent optimization methods that work well are <code class="language-plaintext highlighter-rouge">gradient-based</code>, which simply stated means you find the most promising “direction” for the model to update itself, and take a small step in that direction with every training iteration.</p>

<p>This promising direction is the negative gradient, i.e. the <a href="https://en.wikipedia.org/wiki/Derivative">derivative</a> of the loss with respect to the weights of the model. What this means is that the objective we choose has to be <code class="language-plaintext highlighter-rouge">differentiable</code>, or we won’t be able to figure out which direction we need to travel to get better predictions. Hopefully the ubiquity of this relatively simple calculus in nearly <strong>all</strong> modern machine learning would make Isaac Newton very happy (or Gottfried Leibniz if you <a href="https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy">swing that way</a>).</p>
</details>

<p>Let’s look at a very simple 2-layer neural network whose goal is to predict if an image is a cat or not. Don’t get scared by this math, I explain what these things mean below :cat:</p>

\[\begin{align*}
x &amp;\to \text{Image of a cat} \\
y_{true} &amp;= \text{Is this image a cat or not?} \in \{1, 0\} = \{\text{(yes)}, \text{(no)} \} \\\\

z &amp;= f(x) = W_2 \cdot ReLU(W_1 \cdot x) \in\mathbb{R}^d \\
y_{guess} &amp;= \text{softmax}(z) \\
\end{align*}\]

<p>To explain this scary notation: \(\textcolor{gray}{f}\) is our neural encoder from the <a href="#how-do-we-represent-data-for-computers">Representation Learning</a> section, which we choose to model as the simple 2-layer Neural Network above. We have two layers \(\textcolor{gray}{W_1}\) and \(\textcolor{gray}{W_2}\) with a Rectified Linear Unit (ReLU) as an <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> in the middle (don’t worry too much about this, it just gives our network a little more ‘capacity’ to learn potentially better functions \(\textcolor{gray}{f}\)). Running our input image through this network gives us our learned representation \(\textcolor{gray}{z}\), a sequence of \(\textcolor{gray}{d}\) <a href="https://en.wikipedia.org/wiki/Real_number">real numbers</a> (written in <em>math language</em> as \(\textcolor{gray}{z\in\mathbb{R}^d}\)). Finally, we attach a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> at the end, which will output two probabilities: \(\textcolor{gray}{p_\text{cat}}\) and \(\textcolor{gray}{p_\text{not-cat}}\), where \(\textcolor{gray}{p_\text{cat} + p_\text{not-cat} = 1}\). We consider whichever probability is higher to be our network’s “guess” for whether \(\textcolor{gray}{x}\) was a cat image or not, i.e. \(\textcolor{gray}{y_\text{guess}}\). This process is illustrated visually in the diagram below:</p>

<center><img src="/assets/img/blog/mrl/simple_nn.png" alt="A simple 2-layer neural network computing a guess for whether an image is a cat or not" width="700" height="300" /></center>
<p><br /><br /></p>

<p>Let’s say we choose a good loss to train our model, e.g. a simple <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a> (not important to understand this example). If we test our trained model on 50 unseen cats and 50 unseen dogs, we would hope that \(\textcolor{gray}{y_{guess} = y_{true}}\) on at least 99 of them, to hit our 99% accuracy goal. Take a second to think of what a “bad” model would do in our cat classifier case.</p>

<details><summary>What accuracy will a “bad” binary classification model give?</summary>
<p>The worst model is one that is completely uncertain about its prediction, because it gives us no useful information about our data. For binary classification, i.e. with two classes (cat vs. not cat), complete uncertainty means our model guesses randomly between cat and not cat, i.e. \(\textcolor{gray}{\text{accuracy} = 50\%}\). What is a “bad” classifier if we had ten classes?</p>
</details>

<p>Your (suggested) takeaway from this Section should be:</p>
<blockquote>
  <p>Huh, our <strong>human goal</strong> was to classify cats correctly 99% of the time, but our <strong>machine objective</strong> was this scary sounding <code class="language-plaintext highlighter-rouge">cross-entropy</code>. What gives? Why can’t we train our model with a <code class="language-plaintext highlighter-rouge">git-gud-at-cats</code> loss?</p>
</blockquote>

<p>This is the crux of modern optimization methods: a <code class="language-plaintext highlighter-rouge">misalignment</code> between human goals and the methods we use to train models. In our cat case, we hope that the “cross entropy” is a good proxy for our actual human-specified goal. We will consider this <em>misaligned setting</em> for the rest of this article, but I do provide some optional further reading below on very popular recent efforts towards directly <code class="language-plaintext highlighter-rouge">optimizing for human preferences</code>, which is the focus of <code class="language-plaintext highlighter-rouge">my current research</code>.</p>

<details><summary>Click here to read about Modern Efforts to Directly Optimize Human Preferences</summary>
<p>There has been a lot of recent effort towards <strong>directly</strong> aligning large Machine Learning models to human goals, especially in the realm of Large Language Models, with Reinforcement Learning (RL). This <a href="https://huggingface.co/blog/rlhf">excellent blog post</a> from Lambert et. al<d-cite key="lambert2022illustrating"></d-cite> walks through Reinforcement Learning from Human Feedback (RLHF), which is currently the most popular alignment technique. For Computer Vision nerds, this <a href="https://twitter.com/giffmana/status/1626695378362945541">excellent recent work</a> from Pinto et. al<d-cite key="pinto2023tuning"></d-cite> applies RL techniques to optimize models directly for vision tasks, such as object detection and image captioning.</p>
</details>

<h1 id="matryoshka">Matryoshka</h1>

<p>Alright, so you now hopefully have a basic understanding of</p>
<ol>
  <li>How we use modern Deep Learning methods to learn good representations of data (<a href="#representation-learning">Representation Learning</a>)</li>
  <li>Why we train Neural Encoders with proxy loss functions: the <em>faith</em> that we will achieve our human-interpretable goals, without directly optimizing for them (<a href="#practical-ml-training">Practical ML Training</a>)</li>
</ol>

<p>I’ll now talk about a slightly different problem in modern Machine Learning:</p>
<blockquote>
  <p>How can we learn the generally “best” representation for some given data, and does one even exist?</p>
</blockquote>

<details><summary>Click here to Think more about this Question</summary>
<p>But first, another question: instead of training our simple <a href="#practical-ml-training">cat classifier example</a> above, can we just use a large <a href="https://en.wikipedia.org/wiki/Foundation_model">“foundation” model</a> that someone has already trained that has a good understanding of animals, and somehow <em>transfer</em> that knowledge to directly guess whether an image is a cat? (Some examples include Google’s <a href="https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html">ViT</a>, Meta’s <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">DINO</a>, OpenAI’s <a href="https://openai.com/research/clip">CLIP</a> and Microsoft’s <a href="https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/">LLaVa</a>). This process is called <a href="https://www.v7labs.com/blog/transfer-learning-guide">transfer learning</a>, and is a huge part of what makes modern Deep Learning accessible to researchers and smaller companies with limited resources: we can’t all afford to <a href="https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=7f09d5e24af7">spend millions of dollars</a> training our models!</p>

<p>So how DO we know how “good” a representation is? Do we just have faith in our corporate overlords? Maybe let’s use that as a backup option. Let’s instead define a notion of “goodness” which is directly tied to tasks we care about; after all, we want our representations to be practically useful. For example, the <a href="https://google-research.github.io/task_adaptation/">Visual Task Adaptation Benchmark</a> (VTAB) is a suite of 19 tasks designed to test how generally “good” a visual representation is on things it has not been trained on, which is sometimes called <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture">generalizability</a> or <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of representations. This is a great starting point, i.e. exhaustive benchmarking and evaluation! <code class="language-plaintext highlighter-rouge">Is this our holy grail, the "best" representation</code>?</p>

<p>Spoiler: it’s not quite that simple, because we can’t exhaustively benchmark our representation in <em>all</em> tasks and scenarios!</p>
</details>

<h2 id="what-led-to-creating-mrl">What Led to Creating MRL?</h2>

<h3 id="ml-researcher-bonus-inklings-of-nested-codebook-subspaces">ML Researcher Bonus: Inklings of Nested Codebook Subspaces</h3>
<p>Hold on a second, nested subspaces? Codebooks? What does this all mean? Let’s first <a href="#codebooks-to-embeddings">look at what MRL is</a> under the hood, and circle back here once we’ve understood how MRL works.</p>

<blockquote>
  <p>Have you circled back? Yay! Think of MRL as extending LLC from learning \(\textcolor{gray}{k}\)-bit binary codes to \(\textcolor{gray}{d}\)-dimensional embeddings.
See the <a href="#the-marginal-utility-of-increasing-information">understanding why MRL works</a> section for a walkthrough of what <code class="language-plaintext highlighter-rouge">nested subspaces</code> are, with the help of Calvin and Hobbes!</p>
</blockquote>

<p>The motivation for MRL was sparked from work in 2021 by <a href="https://homes.cs.washington.edu/~kusupati/">Kusupati</a> et. al on learning low-dimensional codebooks (LLC)<d-cite key="kusupati2021llc"></d-cite>. LLC learns representations that are tiny binary codes, i.e. \(\textcolor{gray}{z}\) is just \(\textcolor{gray}{k}\) <a href="https://en.wikipedia.org/wiki/Bit">bits</a> instead of \(\textcolor{gray}{d}\) real-valued numbers. LLC was able to train <code class="language-plaintext highlighter-rouge">nested codebooks</code> with \(\textcolor{gray}{k \in(20, 25, 30)~\text{bits}}\) instead of training three independent codebooks. We could then, for example, easily just use a \(\textcolor{gray}{k=23}\) bit codebook for our data, even though we did not train to learn representations at this value of \(\textcolor{gray}{k}\).</p>

<blockquote>
  <p>To restate this more simply, we were <code class="language-plaintext highlighter-rouge">too lazy to train multiple models</code> with different \(\textcolor{gray}{k}\), so we decided to train just one that worked at all \(\textcolor{gray}{k}\).</p>
</blockquote>

<center>
<blockquote class="twitter-tweet" data-theme="dark"><p lang="en" dir="ltr">🤯I still cannot believe that a runaway para (pg 9) in our old paper (LLC, NeurIPS&#39;21) lead to all the Matryoshka works🪆<br /><br />At that time, I did it as I didn&#39;t have compute to train 3 models 🤣<br /><br />Last couple of weeks have been surreal, thanks everyone!<br /><br />LLC: <a href="https://t.co/gRL6xCOhhP">https://t.co/gRL6xCOhhP</a> <a href="https://t.co/6eSr3w38jQ">pic.twitter.com/6eSr3w38jQ</a></p>&mdash; Aditya Kusupati (@adityakusupati) <a href="https://twitter.com/adityakusupati/status/1757651126273585447?ref_src=twsrc%5Etfw">February 14, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<h3 id="codebooks-to-embeddings">Codebooks to Embeddings</h3>

<p>Recall that <a href="#how-do-we-represent-data-for-computers">we said above</a> that the representation or embedding \(\textcolor{gray}{z}\) learned by our Neural Encoder \(\textcolor{gray}{f}\) for our input data \(\textcolor{gray}{x}\) is a sequence of \(\textcolor{gray}{d}\) numbers, i.e. \(\textcolor{gray}{z = f(x) \in \mathbb{R}^d}\). I now ask you the question that led to Matryoshka Representation Learning:</p>

<blockquote>
  <p>What is the best choice of representation dimensionality \(\textcolor{gray}{d}\) to learn a “good” representation? And is this the same value for all kinds of data? 
If your answer to this question was <code class="language-plaintext highlighter-rouge">Hmm probably not</code> then your thought process is exactly where we (the MRL authors) were in Late 2021.</p>
</blockquote>

<p>Let’s illustrate this idea concretely with an example from the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>, Figure 9a. The leftmost image in the row is <strong>GT: Sweatshirt</strong>, which is the <em>Ground Truth</em> (GT) of the data, i.e. what we consider the <em>true</em> label, \(\textcolor{gray}{y_\text{true} =}\) <strong>Sweatshirt</strong>. You can think of the other 4 images as what the model is “\(\textcolor{green}{\text{looking at}}\)” to make a decision about what this image represents.  Each of these 4 images is using a different \(\textcolor{orange}{d}\)-dimensional representation of the image to make its decision, with \(\textcolor{gray}{d \in} (\textcolor{orange}{8, 16, 32, 2048})\), and the predicted label \(\textcolor{gray}{y_\text{pred}}\) above each image. We can think of a larger \(\textcolor{orange}{d}\) value as being able to represent “more information” about the image (because there are more numbers to represent this information!)</p>

<center><img src="/assets/img/blog/mrl/mrl-embedding-capacity.png" alt="Demonstrating the embedding capacity required by varying complexity of images" width="700" height="200" /></center>
<p><br /><br /></p>

<p>As we can see, with very small dimensionality \(\textcolor{orange}{d}\), the model makes a mistake and thinks the image is <strong>Sunglasses</strong>, which we can see with \(\textcolor{green}{\text{where the model is looking}}\). When we increase \(\textcolor{orange}{d=32}\), the model is able to shift its focus more to <strong>Sweatshirt</strong> and get the prediction correct, and it stays correct until \(\textcolor{orange}{d=2048}\). This means we could have easily just used a \(\textcolor{gray}{\dfrac{2048}{32} = 64\times}\) smaller embedding to correctly predict this image!</p>

<p>It makes sense to use the smallest \(\textcolor{gray}{d}\) that works for every data point, because we can save memory (less numbers = less memory) and run faster inference, i.e. compute \(\textcolor{gray}{y_\text{guess}}\) as shown in the <a href="#practical-ml-training">ML Training</a> section.</p>

<p>There’s one problem: machine learning models are trained with fixed representation dimensionality \(\textcolor{gray}{d}\). For ResNet-50<d-cite key="he2016deep"></d-cite>, an extremely popular CNN, \(\textcolor{gray}{d=2048}\). For OpenAI’s <a href="https://openai.com/blog/new-embedding-models-and-api-updates">latest embedding model</a> <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code>, \(\textcolor{gray}{d=3072}\). If we want a smaller \(\textcolor{gray}{d}\), the prevalent methods were to use traditional <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction">dimensionality reduction</a> techniques, such as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a>. The problem with these methods is that they operate “post-hoc” after our \(\textcolor{gray}{d}\)-dimensional embedding as been learned by our Neural Encoder, and are thus not data-aware or learned. Is there a way to automatically learn these lower dimensional embeddings without training separate models every time?</p>

<h4 id="tldr-2">tl;dr</h4>
<blockquote>
  <p>Can we encourage our own laziness (or lack of GPUs :pensive:) to train many independent models and <code class="language-plaintext highlighter-rouge">just train one model instead</code>?</p>
</blockquote>

<h2 id="what-is-mrl-really-this-time">What is MRL? (Really this Time)</h2>
<p>MRL learns these lower-dimensional embeddings baked into the original embedding, just like a series of Matryoshka Dolls! For example, the \(\textcolor{red}{\text{smallest doll}~z_{1:8}}\) has representation dimensionality \(\textcolor{red}{d=8}\), which sits inside a \(\textcolor{orange}{\text{slightly larger doll}~z_{1:16}}\) with \(\textcolor{orange}{d=16}\), which sits inside an \(\textcolor{blue}{\text{even larger doll}~z_{1:32}}\), until we reach the \(\textcolor{Gray}{\text{\textbf{largest doll}}~z_{1:2048}}\) with \(\textcolor{gray}{d=2048}\), as seen in the figure below. Hereon, I will interchangeably use <code class="language-plaintext highlighter-rouge">dolls</code> to refer to <code class="language-plaintext highlighter-rouge">representations learned by MRL</code>.</p>

<center><img src="/assets/img/blog/mrl/mrl-method.png" alt="Demonstrating the MRL methodology" width="600" height="300" /></center>
<p><br /><br /></p>

<details><summary>What are those symbols under “Training”?</summary>
<p>MRL is primarily a training paradigm to learn a nested structure of representations, resembling Matryoshka dolls. So how do we train a model to enforce this structure? It’s actually surprisingly simple! We apply the same <code class="language-plaintext highlighter-rouge">cross-entropy loss</code> we would have used for a plain old regular model (just the \(\textcolor{Gray}{\text{\textbf{largest doll}}~z_{1:2048}}\)) to each doll independently, and average all these losses together:</p>

\[\begin{align*}
\mathcal{L_\text{Regular}} &amp;= \mathcal{L}(z_{1:2048}) \\
\mathcal{L_\text{Matryoshka}} &amp;= \text{average}\left(\mathcal{L}(z_{1:8}) + \mathcal{L}(z_{1:16}) + ... + \mathcal{L}(z_{1:2048})\right)
\end{align*}\]

<p>This simple modification forces the model to learn dolls that are valid and useful representations of the data by themselves (otherwise that specific doll’s loss would be high!) This means we can freely use whichever doll fits our purpose (budget vs accuracy).</p>
</details>

<h2 id="how-good-is-mrl">How Good is MRL?</h2>
<p>You may be wondering, how does learning Matryoshka dolls compare to training a new doll from scratch at different dimensionality \(\textcolor{gray}{d}\) every time? While training all dolls at once with MRL is much more efficient, surely each MRL doll’s performance will be <strong>worse</strong> than than its corresponding independently trained doll?</p>

<p>We were pleasantly surprised to discover that \(\textcolor{blue}{\text{MRL dolls}}\) outperform \(\textcolor{green}{\text{independently trained dolls}}\) at each dimensionality, as seen in the figures below from the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>, at both:</p>

<p>(a) <code class="language-plaintext highlighter-rouge">Million scale</code> on <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet-1K</a> with a ResNet-50<d-cite key="he2016deep"></d-cite> Neural Encoder at \(\textcolor{gray}{d_\text{ImageNet} \in (8, 16, ... , 2048)}\)</p>

<p>(b) <code class="language-plaintext highlighter-rouge">Billion scale</code> on JFT-300M<d-cite key="sun2017revisiting"></d-cite> with ViT B/16<d-cite key="dosovitskiy2020image"></d-cite> and <a href="https://blog.research.google/2021/05/align-scaling-up-visual-and-vision.html?m=0">ALIGN</a> Neural Encoders with \(\textcolor{gray}{d_\text{JFT} \in (12, 24, ... , 768)}\)</p>

<center>
<img src="/assets/img/blog/mrl/mrl-r50-imagenet.png" alt="MRL with ResNet50 models on ImageNet show strong performance at all doll sizes" width="400" height="300" />
<p>a) ResNet-50 1-NN Accuracy on ImageNet</p>
<img src="/assets/img/blog/mrl/mrl-vit-align-jft.png" alt="MRL performance seamlessly scales to billion scale data" width="400" height="300" />
<p>b) ViT B/16 and ALIGN 1-NN Accuracy on JFT</p>
</center>

<p>In summary, MRL provides little to no accuracy drop for large efficiency gains across:</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">Data Scale</code> - million to billion</li>
  <li><code class="language-plaintext highlighter-rouge">Data Modality</code> - vision, language, vision + language</li>
  <li><code class="language-plaintext highlighter-rouge">Neural Encoder Architecture</code> - ResNet-50, ConvNeXt<d-cite key="liu2022convnet"></d-cite>, ViT, <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>, ALIGN</li>
</ol>

<h2 id="why-does-mrl-work">Why Does MRL Work?</h2>
<p>I’m going to be frank with you: <code class="language-plaintext highlighter-rouge">we still don't know for sure</code>, but we have a good hypothesis: MRL learns a coarse-to-fine hierarchy of nested subspaces, and efficiently packs information in all vector spaces while being explicitly trained only at logarithmic granularities (\(\textcolor{gray}{d_{ImageNet}}\) and \(\textcolor{gray}{d_{JFT}}\)). As long as we learn a <a href="https://www.pinecone.io/learn/series/nlp/dense-vector-embeddings-nlp/">dense vector representation</a> \(\textcolor{gray}{z}\), MRL just works! Let’s motivate this intuition with some experiments, shall we?</p>

<h3 id="the-marginal-utility-of-increasing-information">The Marginal Utility of Increasing Information</h3>
<p><a href="#codebooks-to-embeddings">Recall</a> that MRL was born from the question:</p>
<blockquote>
  <p>How much information do we need to represent our data well, i.e. how big does \(\textcolor{gray}{d}\) need to be?</p>
</blockquote>

<p>We hypothesize that MRL’s nested loss \(\textcolor{gray}{\mathcal{L_\text{Matryoshka}}}\) (see “symbols under training” <a href="#what-is-mrl-really-this-time">here</a>) enforces a <a href="https://en.wikipedia.org/wiki/Linear_subspace">vector subspace</a> structure - each learned representation vector \(\textcolor{gray}{z\in\mathbb{R}^d}\) lies in a \(\textcolor{gray}{d}\)-dimensional vector space that is a subspace of larger vector space. Woah hold on, that made no sense to me. Why are you throwing math-y jargon at me? Let’s simplify this with an example:</p>

<center><img src="/assets/img/blog/mrl/calvin-1d.png" alt="1-D Calvin lies in a subspace - where 2-D Calvin lives" width="700" height="500" /></center>
<p><br /><br /></p>

<p>I’m going to use an analogy from my favorite comic book series (sorry <a href="https://twitter.com/xkcd?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">xkcd</a>, you’re close behind). Think of the characters of Calvin and Hobbes as vectors lying in 3D space (i.e. the vector space \(\textcolor{gray}{\mathbb{R}^3}\)), just like you and I are in the real 3D world (quick caveat: Bill Watterson has <code class="language-plaintext highlighter-rouge">projected</code> the world of Calvin &amp; Hobbes to 2D space to draw them in comic books, because we unfortunately don’t have 3D comic books quite yet! Or wait, are animated movies 3D comics? :exploding_head:)</p>

<p>In the comic strip above, Calvin suddenly wakes up to find himself existing only in the 2D vector space \(\textcolor{gray}{\mathbb{R}^2}\), while the rest of his world is still in 3D! We can see how difficult this kind of existence would be in the rest of the strip. Calvin has woken up in the 2D subspace of 3D vector subspace, as he exists in both 2D and 3D! Come to think of it, all 2D objects (like Calvin) also exist in 3D space, don’t they? This is exactly the concept of the vector subspace structure that we hypothesize MRL is enforcing: vectors embedded with MRL lie in nested subspaces - e.g. for ImageNet, vectors lie in subspaces from \(\textcolor{gray}{\mathbb{R}^8}\) to \(\textcolor{gray}{\mathbb{R}^{2048}}\).</p>

<blockquote>
  <p>Now how can we examine if this hypothesis has any validity? Unfortunately, we can’t visualize vector spaces in 4D or higher (if you can, please let me know). What can we do?</p>
</blockquote>

<h3 id="low-dimensional-visualizations-of-mrl-vector-spaces">Low-dimensional Visualizations of MRL Vector Spaces</h3>
<p>Luckily for us dimensionality reduction techniques like <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> and <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a> are the answer! Generally speaking, these techniques try to preserve relationships between vectors in high-d vector spaces (e.g. \(\textcolor{gray}{\mathbb{R}^{3072}}\)) in low-d subspaces that we can actually visualize (i.e. \(\textcolor{gray}{\mathbb{R}^{2}}\) or \(\textcolor{gray}{\mathbb{R}^{3}}\))</p>

<h4 id="1-pca">1. PCA</h4>
<p><a href="https://twitter.com/ZainHasan6">Zain Hasan</a> created a <a href="https://twitter.com/ZainHasan6/status/1757810956103533045?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1757867522144002255%7Ctwgr%5E52f9bb2d416af075cbf225e2b2a0f43fac01d777%7Ctwcon%5Es3_&amp;ref_url=http%3A%2F%2Flocalhost%3A4000%2Fblog%2F2024%2Fmrl%2F">simple and clean visualization</a> of MRL vectors encoded into both 2D and 3D subspaces with PCA as we increase dimensionality of the high-d vector space, which you can see below. As seen in the visualization, the structure of data is quite consistent after \(\textcolor{gray}{\mathbb{R}^{512}}\) and barely changes after \(\textcolor{gray}{\mathbb{R}^{2000}}\). This means that we can capture a significant portion of the structure of vectors in \(\textcolor{gray}{\mathbb{R}^{512}}\), whose vectors are only 16% as large as the vectors from the largest vector space \(\textcolor{gray}{\mathbb{R}^{3072}}\)!</p>

<center>
<blockquote class="twitter-tweet" data-theme="dark" data-media-max-width="560"><p lang="en" dir="ltr">More visualizations helping me understand my own paper 🤪 amazing work <a href="https://twitter.com/ZainHasan6?ref_src=twsrc%5Etfw">@ZainHasan6</a> ! <br /><br />When creating MRL, we thought about the marginal utility of increasing dims (e.g. what really changes if we jump from 64-d to 128-d? How much additional information do we get?)<br /><br />(1/n) <a href="https://t.co/SEQeHiLWol">https://t.co/SEQeHiLWol</a></p>&mdash; Aniket Rege (@wregss) <a href="https://twitter.com/wregss/status/1757867522144002255?ref_src=twsrc%5Etfw">February 14, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p>This visualization agrees with the accuracy saturation we observed on ImageNet and JFT (see <a href="#how-good-is-mrl">How Good is MRL</a>), which were aggregate (average) numbers. This visualization indicates that there is actually a <code class="language-plaintext highlighter-rouge">geometric subspace structure</code> that backs up the diminishing returns of accuracy as we increase embedding dimensionality.</p>

<h4 id="2-t-sne">2. t-SNE</h4>

<p>The folks at <a href="https://home.nomic.ai/">Nomic AI</a> shared this <a href="https://blog.nomic.ai/posts/nomic-embed-matryoshka">excellent interactive visualization</a> (go play with it!) of MRL embeddings on their own Nomic dataset with a <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>:</p>

<center><img src="/assets/img/blog/mrl/mrl-tsne-nomic.gif" alt="A gif illustrating Adaptive Retrieval with MRL from Xenova" width="640" height="320" /></center>
<p><br /><br /></p>

<p>As you can see, the structure of the data is fairly consistent from a low-d subspace \(\textcolor{gray}{\mathbb{R}^{128}}\) to the largest vector space \(\textcolor{gray}{\mathbb{R}^{768}}\), differing only by a rotation. While t-SNE clustering should be taken with a grain of salt (data clustered in t-SNE visualizations may not necessarily be clustered in high-d), this visualization provides evidence about the <em>similarity in subspace structure</em> as we reduce the subspace dimensionality \(\textcolor{gray}{d: 768\to 128}\), i.e. we can capture a good amount of the structure of data with \(\textcolor{gray}{d=128}\), we don’t need the largest vector space with \(\textcolor{gray}{d=768}\)!</p>

<blockquote>
  <p>In summary, the marginal utility of jumping to a higher-dimensional vector space quickly saturates, and MRL helps you find the sweet spot!</p>
</blockquote>

<h1 id="using-mrl">Using MRL</h1>

<h2 id="web-scale-search">Web-Scale Search!</h2>
<p>Now what can we do with this collection of dolls? You might have gotten a hint from the diagram <a href="#finally-enter-mrl">above</a>, but if not, <code class="language-plaintext highlighter-rouge">Matryoshka</code> enables a strong information retrieval setup, just like how Google shows you your favorite cat pics when you search for them! Here’s a walkthrough of what that might look like:</p>

<h3 id="encode-your-data">Encode your Data</h3>
<p>Use the largest doll that fits in your budget to encode all cat pictures on the internet into a database \(\textcolor{gray}{X_{Mat}}\). For example, say you have 50 Million cat images (please share them with me) and 100 Gigabytes of storage. With <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">fp32 precision</a></p>
<ol>
  <li>The smallest doll \(\textcolor{gray}{d=8}\) would occupy \(\textcolor{gray}{8 * 4 = 32}\) bytes of memory, and thus \(\textcolor{gray}{X_{Mat}}\) would occupy \(\textcolor{gray}{50,000,000 * 32 = 400}\) Megabytes on disk. We are only using 0.4% of our 100GB budget, we can go much higher!</li>
  <li>The largest doll \(\textcolor{gray}{d=2048}\) would occupy \(\textcolor{gray}{2048 * 4 = 8192}\) bytes of memory, and thus \(\textcolor{gray}{X_{Mat}}\) would occupy \(\textcolor{gray}{50,000,000 * 8192 = 409.6}\) Gigabytes on disk. The largest doll is too big to fit into our 100 GB memory budget, we need something in the middle!</li>
  <li>With some <a href="https://www.youtube.com/watch?v=3M_5oYU-IsU&amp;ab_channel=MichaelDapaah">quick mafs</a> calculations, we see that we can use a doll of size \(\textcolor{gray}{d=500}\), since \(\textcolor{gray}{X_{Mat}}\) would occupy \(\textcolor{gray}{50,000,000 * 500 * 4 = 100}\) Gigabytes exactly.</li>
</ol>

<h3 id="build-a-search-index">Build a Search Index</h3>
<p>Now that we have encoded our data with MRL, we need a way to search for relevant cat pictures, using a <code class="language-plaintext highlighter-rouge">search index</code> built on top of our database \(\textcolor{gray}{X_{Mat}}\)! To speak more corporately, Meta uses hierarchical graph indices<d-cite key="malkov2018efficient"></d-cite>, Microsoft uses hybrid SSD-Disk graph indices<d-cite key="jayaram2019diskann"></d-cite>, Google uses tree indices<d-cite key="sivic2003video"></d-cite> and sophisticated Vector Quantization techniques<d-cite key="guo2020accelerating"></d-cite>, to name a few.</p>

<h3 id="searching-a-query">Searching a Query</h3>
<p>Let’s say we want to find the best “Derpy Orange Cat” in our database. We’ll use the search index to find the 50 closest matches (in ML terminology, “Nearest Neighbors”), and hopefully serve a very happy customer! MRL has enabled us to <code class="language-plaintext highlighter-rouge">use a high-dimensional embedding "for free"</code>, since we didn’t have to train or finetune a separate 500-dimensional doll to encode our database, or use dimensionality reduction methods on our largest doll.</p>

<h3 id="hold-on-a-minute-can-we-just-use-any-doll">Hold on a Minute, Can we Just Use any Doll?</h3>
<blockquote>
  <p>You may have already asked the question: Hey, we only trained MRL at specific doll sizes \(\textcolor{gray}{d_{ImageNet}}\) and \(\textcolor{gray}{d_{JFT}}\). Can we just use any doll of size \(\textcolor{gray}{d}\) that doesn’t lie in these specific values we used to train the MRL model?</p>
</blockquote>

<center>
<img src="/assets/img/blog/mrl/mrl-interpolate.png" alt="Demonstrating MRL's interpolation behavior at dimensionalities it was not trained on" width="400" height="300" />
<p> MRL model accuracies interpolate! </p>
</center>
<p><br /><br /></p>

<p>It turns that yes, you can - <code class="language-plaintext highlighter-rouge">MRL model accuracies seamlessly interpolate at all doll sizes</code> between the fixed doll sizes it was trained for (\(\textcolor{gray}{d_{ImageNet}}\) and \(\textcolor{gray}{d_{JFT}}\))! You can see this in the figure from the MRL paper above, where the X-Axis is the doll size or representation size, and all the \(\textcolor{red}{\text{red points}}\) are evaluations at \(\textcolor{red}{\text{interpolated sizes}}\). This means we can freely, in the <a href="https://openai.com/blog/new-embedding-models-and-api-updates">words of OpenAI</a>, “remove some numbers from the end of the sequence” of any representation, and use that embedding directly! I’m not going to make a doll analogy for this because the thought is quite gruesome.</p>

<h2 id="adaptive-retrieval">Adaptive Retrieval</h2>
<p>A very simple yet powerful use-case that extends the simple web-scale search example above is to use MRL for retrieval <em>adaptively</em>, i.e.</p>
<ol>
  <li>Shortlist a large pool of 1000 cat candidates with small \(\textcolor{gray}{d=32}\)</li>
  <li>Reorder/Rerank the shortlisted pool of cats with large \(\textcolor{gray}{d=512}\)</li>
</ol>

<p>The reason this is easy with MRL is that the embedding for shortlisting is just a chopped-off version of the embedding for re-ordering! We thus don’t need to store multiple large copies of \(\textcolor{gray}{X_{Mat}}\) or query multiple search indices built on different \(\textcolor{gray}{d}\) (very slow).</p>

<p>If this sounds interesting, we showed a very simple and powerful Adaptive Retrieval technique called Funnel in the original paper, and wrote a whole new paper on Adaptive Representations for Approximate Nearest Neighbor Search (AdANNS<d-cite key="rege2023adanns"></d-cite>) - using MRL to add adaptivity to <code class="language-plaintext highlighter-rouge">every component of web-scale search</code>- stay tuned for more my next blog post about this!</p>

<center><img src="/assets/img/blog/mrl/AR-mrl-xenova.gif" alt="A gif illustrating Adaptive Retrieval with MRL from Xenova" width="480" height="320" /></center>
<p><br /><br /></p>

<p>There have been several excellent resources emerging on twitter walking through MRL-powered adaptive retrieval, including:</p>
<ol>
  <li>This <a href="https://ujjwalm29.medium.com/matryoshka-representation-learning-a-guide-to-faster-semantic-search-1c9025543530">excellent blog post</a> from <a href="https://twitter.com/ujjwalm29">Ujjwal Maheshwari</a> walks through the methodology and code of MRL-style Adaptive Retrieval.</li>
  <li><a href="https://twitter.com/xenovacom/status/1757798436009599413">A cool demo</a> from <a href="https://twitter.com/xenovacom">Xenova</a> showing that the highest dimensionality (e.g. \(\textcolor{gray}{d=768}\) for BERT-Base) is not required to embed data - have a look at what happens in the gif above if you drop from \(\textcolor{gray}{d=768}\) all the way down to \(\textcolor{gray}{d=64}\) with MRL :nesting_dolls:</li>
  <li><a href="https://twitter.com/ggrdson">Greg Richardson</a>, <a href="https://twitter.com/egor_test">Egor Romanov</a> and <a href="https://twitter.com/kiwicopple">Paul Cooplestone</a> of <a href="https://twitter.com/Supabase">Supabase</a> wrote a <a href="https://supabase.com/blog/matryoshka-embeddings">comprehensive blog</a> on Adaptive Retrieval with the new OpenAI embedding models, with some interesting insights into approximate search with HNSW<d-cite key="malkov2018efficient"></d-cite> as well - this is a sneak peak into our follow-up work to MRL - AdANNS<d-cite key="rege2023adanns"></d-cite>.</li>
</ol>

<h2 id="so-what-is-the-catch">So What is the Catch?</h2>
<p>None! Please go train MRL models on huge datasets with huge transformers and open source your work!</p>

<p>And that, dear reader, is the biggest catch: MRL models have a <code class="language-plaintext highlighter-rouge">one-time cost of retraining from scratch</code> to match independently trained models. We discovered that this can be alleviated to a large extent by unfreezing some of the last layers of the Neural Encoder and finetuning for several epochs (See Table 26 in the MRL paper<d-cite key="kusupati2022matryoshka"></d-cite>), but this does not recover from-scratch training performance fully, especially for smaller dolls (\(\textcolor{gray}{d \leq 32}\)).</p>

<h2 id="open-source-mrl-models">Open-Source MRL Models</h2>
<p>Here are some freely available open-source MRL models to play around with!</p>
<ol>
  <li>ResNet-18/34/50/101 MRL and independent baseline models trained on ImageNet-1K: <a href="https://huggingface.co/aniketr/mrl-resnet50">huggingface.co/aniketr/mrl-resnet50</a></li>
  <li>ConvNeXt-Tiny trained on ImageNet-1K: <a href="https://huggingface.co/aniketr/mrl-convnext-tiny">huggingface.co/aniketr/mrl-convnext-tiny</a></li>
  <li>BERT-Base models finetuned on Natural Questions: <a href="https://huggingface.co/aniketr/mrl-nq">huggingface.co/aniketr/mrl-nq</a></li>
  <li><a href="https://home.nomic.ai/">Nomic AI</a>’s long-context BERT model by: <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">huggingface.co/nomic-ai/nomic-embed-text-v1.5</a></li>
</ol>

<h2 id="thoughts-get-in-touch">Thoughts? Get in Touch!</h2>
<p>I know this was quite a long article, so thanks for sticking around, and I hope you got something useful out of it!</p>

<p>If you’re an ML or CS researcher and have thoughts or questions about our work or improving this article, I would love to have chat about <code class="language-plaintext highlighter-rouge">MRL</code>, our followup work, and my current research.</p>

<p>If you’re someone who is generally interested in our work and found this article interesting, difficult, or relevant, I’d love to hear from you too!</p>

<p>Please get in touch with me via <code class="language-plaintext highlighter-rouge">aniketr@cs.wisc.edu</code> or join the discussion on my Twitter:</p>

<center>
<blockquote class="twitter-tweet" data-theme="dark"><p lang="en" dir="ltr">With all the recent hype around 🪆Matryoshka Representation Learning 🪆(Thanks <a href="https://twitter.com/OpenAI?ref_src=twsrc%5Etfw">@OpenAI</a> !), I finally put my longstanding plan of writing a detailed blog about MRL to action<a href="https://t.co/guGZRSxxDo">https://t.co/guGZRSxxDo</a><br /><br />This blog is NOT a paper walkthrough (see <a href="https://twitter.com/RitvikRastogi19?ref_src=twsrc%5Etfw">@RitvikRastogi19</a> for that!)<br /><br />(1/7)</p>&mdash; Aniket Rege (@wregss) <a href="https://twitter.com/wregss/status/1755266172868571597?ref_src=twsrc%5Etfw">February 7, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<h2 id="further-reading">Further Reading</h2>
<ol>
  <li><a href="https://arxiv.org/abs/2106.01487">LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</a> - learning low-dimensional binary codes for classes and instances for data, e.g. class = “Dog” and instance = a specific German Shepherd Image.</li>
  <li><a href="https://arxiv.org/abs/2305.19435">AdANNS: A Framework for Adaptive Semantic Search</a> - using adaptive representations (via MRL) to flexibly decouple all stages of a large scale search system and provide strong accuracy-compute tradeoff for all deployment budgets. Currently in use in Google Products.</li>
  <li><a href="https://arxiv.org/abs/2310.07707">MatFormer: Nested Transformer for Elastic Inference</a> - using MRL in the weight space of a Transformer to extract hundreds of smaller transformers after a single training pass.</li>
  <li><a href="https://www.pinecone.io/learn/series/faiss/">Pinecone blogs on Vector Search Components</a> - a series of excellent blog posts by <a href="https://twitter.com/jamescalam">James Briggs</a> on the various components of vector search at scale, including search space pruning and vector quantization techniques.</li>
  <li>Excellent blog posts from <a href="https://blog.nomic.ai/posts/nomic-embed-matryoshka">Nomic AI</a> on their high quality open-source MRL embedding model, <a href="https://twitter.com/andreer">Andreas Eriksen</a> at <a href="https://blog.vespa.ai/matryoshka-embeddings-in-vespa/">Vespa AI</a> on easily using OpenAI’s MRL embeddings with Vespa.</li>
</ol>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://harshali.in/about">Harshali Paralikar</a>, <a href="https://sites.google.com/view/gbhatt/">Gantavya Bhatt</a>, and <a href="https://homes.cs.washington.edu/~kusupati/">Aditya Kusupati</a> for their feedback in editing and improving this article.</p>]]></content><author><name>Aniket Rege</name></author><category term="ml" /><category term="embeddings" /><category term="flexibility" /><summary type="html"><![CDATA[What do these scary sounding words mean?]]></summary></entry></feed>